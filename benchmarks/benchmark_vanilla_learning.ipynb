{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e276dbe",
   "metadata": {},
   "source": [
    "# Benchmark Vanilla GP Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36444a5",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d17d1f",
   "metadata": {},
   "source": [
    "This notebook provides an example of how to perform Gaussian Process Regression on a 1D manifold. In this example we consider a supervised learning scenario, namely the number of labeled data points is equivalent to the number of the sampled points from the underlying manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e9afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from manifold_gp.models.vanilla_gp import VanillaGP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75701656",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02483bf0",
   "metadata": {},
   "source": [
    "### Load & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af216c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike, buzz_tomshardware, buzz_twitter, ctslices, elevators, protein, song, mnist, mnist_single\n",
    "dataset = 'ctslices'\n",
    "samples_split = 0.9\n",
    "preprocess = False\n",
    "normalize_features = False\n",
    "normalize_labels = True\n",
    "\n",
    "data = np.load('../datasets/'+dataset+'.npy')\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "idx = np.random.permutation(x.shape[0])\n",
    "x = x[idx, :]\n",
    "y = y[idx]\n",
    "\n",
    "num_samples = int(samples_split * len(data))\n",
    "sampled_x, sampled_y = x[:num_samples, :], y[:num_samples]\n",
    "test_x, test_y = x[num_samples:, :], y[num_samples:]\n",
    "\n",
    "# # Map features to [-1, 1] (used in gpytorch)\n",
    "# x = data[:, :-1]\n",
    "# x = x - x.min(0)[0]\n",
    "# x = 2 * (x/ x.max(0)[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess:\n",
    "    # # remove coincident points\n",
    "    # sampled_x, id_unique = np.unique(sampled_x, axis=0, return_index=True)\n",
    "    # sampled_y = sampled_y[id_unique]\n",
    "\n",
    "    # cut between x% and y% percentile of distances\n",
    "    num_avg = 1\n",
    "    p_start, p_end = 0.1, 0.9\n",
    "    num_samples = sampled_x.shape[0]\n",
    "    \n",
    "    import faiss\n",
    "    res = faiss.StandardGpuResources()\n",
    "    knn = faiss.GpuIndexIVFFlat(res, sampled_x.shape[1], 1, faiss.METRIC_L2)\n",
    "    knn.train(sampled_x)\n",
    "    knn.add(sampled_x)\n",
    "    v = np.sqrt(knn.search(sampled_x, num_avg+1)[0][:,1:])\n",
    "    idx = np.argsort(v.mean(axis=1))\n",
    "    idx = np.delete(idx, np.arange(int(num_samples*p_start),int(num_samples*p_end)))\n",
    "    sampled_x = np.delete(sampled_x, idx, axis=0)\n",
    "    sampled_y = np.delete(sampled_y, idx)\n",
    "m = sampled_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331daec5",
   "metadata": {},
   "source": [
    "### Trainset & Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163309fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.75 * m)\n",
    "\n",
    "train_x, train_y = sampled_x[:train_split], sampled_y[:train_split]\n",
    "\n",
    "train_x, train_y = torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float()\n",
    "test_x, test_y = torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float()\n",
    "\n",
    "if normalize_features:\n",
    "    mu_x, std_x = train_x.mean(dim=-2, keepdim=True), train_x.std(dim=-2, keepdim=True) + 1e-6\n",
    "    train_x.sub_(mu_x).div_(std_x)\n",
    "    test_x.sub_(mu_x).div_(std_x)\n",
    "    \n",
    "if normalize_labels:\n",
    "    mu_y, std_y = train_y.mean(), train_y.std()\n",
    "    train_y.sub_(mu_y).div_(std_y)\n",
    "    test_y.sub_(mu_y).div_(std_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87581de",
   "metadata": {},
   "source": [
    "### Move Data to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c91b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_x.contiguous(), train_y.contiguous()\n",
    "test_x, test_y = test_x.contiguous(), test_y.contiguous()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "test_x, test_y = test_x.to(device), test_y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3441ad2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e47b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "    noise_constraint=gpytorch.constraints.GreaterThan(1e-8),\n",
    "    noise_prior=None  # NormalPrior(torch.tensor([0.0]).to(device),  torch.tensor([1/9]).sqrt().to(device))\n",
    ")\n",
    "\n",
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "# gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "model = VanillaGP(train_x, train_y, likelihood, kernel).to(device)\n",
    "\n",
    "hypers = {\n",
    "    'likelihood.noise_covar.noise': 1e-2,\n",
    "    'covar_module.base_kernel.lengthscale': 1.0,\n",
    "    'covar_module.outputscale': 1.0,\n",
    "}\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14763b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb0ab30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.86 GiB. GPU 0 has a total capacty of 23.64 GiB of which 2.61 GiB is free. Including non-PyTorch memory, this process has 20.08 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_171102/2286600766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvanilla_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time: %.2g sec\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/manifold_gp/models/vanilla_gp.py\u001b[0m in \u001b[0;36mvanilla_train\u001b[0;34m(self, lr, iter, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             )\n\u001b[0;32m--> 491\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                \"of them.\")\n\u001b[1;32m    273\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/linear_operator/functions/_pivoted_cholesky.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output, _)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Now compute the backward pass of res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Define gradient placeholders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             )\n\u001b[0;32m--> 491\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.86 GiB. GPU 0 has a total capacty of 23.64 GiB of which 2.61 GiB is free. Including non-PyTorch memory, this process has 20.08 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "model.vanilla_train(lr=1e-1, iter=100, verbose=True)\n",
    "t1 = time()\n",
    "print(\"Time: %.2g sec\" % (t1 - t0))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bcc3e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e576d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "likelihood.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab36075",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984df9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.cg_tolerance(10000):\n",
    "    preds_test = likelihood(model(test_x))\n",
    "        \n",
    "    error = test_y - preds_test.mean\n",
    "    covar = preds_test.lazy_covariance_matrix.evaluate_kernel()\n",
    "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=error.unsqueeze(-1), logdet=True)\n",
    "    \n",
    "    rmse = (error.square().sum()/test_y.shape[0]).sqrt()\n",
    "    nll = 0.5 * sum([inv_quad, logdet, error.size(-1)* np.log(2 * np.pi)])/test_y.shape[0]\n",
    "    \n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"NLL: \", nll)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e066e02-d0d6-4cb3-8134-6a519495e1a0",
   "metadata": {},
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.cg_tolerance(1000):\n",
    "    preds_train = likelihood(model(train_x))\n",
    "        \n",
    "    error = train_y - preds_train.mean\n",
    "    covar = preds_train.lazy_covariance_matrix.evaluate_kernel()\n",
    "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=error.unsqueeze(-1), logdet=True)\n",
    "    \n",
    "    rmse = (error.square().sum()/train_y.shape[0]).sqrt()\n",
    "    nll = 0.5 * sum([inv_quad, logdet, error.size(-1)* np.log(2 * np.pi)])/train_y.shape[0]\n",
    "    \n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"NLL: \", nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e5f81-cc04-41da-8b91-b41649f62536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
